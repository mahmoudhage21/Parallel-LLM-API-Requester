{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import tiktoken\n",
    "import threading\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from threading import Semaphore\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Any\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_fixed,\n",
    "    wait_random,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Format Example\n",
    "\n",
    "The input data, `message_sequences`, should be structured as a list of message dictionaries, where each message has a `role` (either \"system\" or \"user\") and `content`. Here’s a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 1 + 1?'}],\n",
       " [{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 2 + 2?'}],\n",
       " [{'role': 'system',\n",
       "   'content': \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
       "  {'role': 'user', 'content': 'What is 3 + 3?'}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_sequences = []\n",
    "for i in range(1, 101):\n",
    "    message_sequences.append([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.  Always answer in the following json format: {'content': '2'}.\"},\n",
    "     {\"role\": \"user\", \"content\": f\"What is {i} + {i}?\"}\n",
    "    ])\n",
    "message_sequences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom TokenSemaphore for Token Rate Limiting\n",
    "\n",
    "The Python standard Semaphore from the threading module starts with an internal counter, which you specify upon creation. This counter decrements each time acquire() is called and increments when release() is called. However, the standard Semaphore doesn't support acquiring or releasing more than one unit of the counter at a time, which means it can't directly manage multiple tokens per request out-of-the-box if those requests consume a **variable** number of tokens.\n",
    "\n",
    "\n",
    "The following custom class allows you to specify how many tokens to acquire or release at a time, giving you the flexibility needed for handling variable token counts per API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenSemaphore:\n",
    "    def __init__(self, max_tokens):\n",
    "        self.tokens = max_tokens\n",
    "        self.lock = threading.Lock()\n",
    "        self.condition = threading.Condition(self.lock)\n",
    "\n",
    "    def acquire(self, required_tokens):\n",
    "        with self.lock:\n",
    "            while self.tokens < required_tokens:\n",
    "                self.condition.wait()\n",
    "            self.tokens -= required_tokens\n",
    "\n",
    "    def release(self, released_tokens):\n",
    "        with self.lock:\n",
    "            self.tokens += released_tokens\n",
    "            self.condition.notify_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIRequester:\n",
    "    def __init__(self, model_name, temperature=1.0, max_tokens=15, rate_limit=20, token_rate_limit=8000):\n",
    "        self.client = AzureOpenAI(\n",
    "                # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "                api_version=\"2024-02-15-preview\",\n",
    "                # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "               azure_endpoint= \"https://chm-openai.openai.azure.com/\",\n",
    "            )\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.rate_limit = rate_limit  # max requests per minute, adjust according to your limit\n",
    "        self.request_semaphore = Semaphore(self.rate_limit)\n",
    "        self.token_rate_limit = token_rate_limit  # Token rate limit per minute\n",
    "        self.token_semaphore = TokenSemaphore(self.token_rate_limit)  # Custom semaphore for token limit\n",
    "    \n",
    "    # Adapted from https://github.com/openai/openai-cookbook/blob/970d8261fbf6206718fe205e88e37f4745f9cf76/examples/api_request_parallel_processor.py#L339-L389\n",
    "    def num_tokens_consumed_from_request(\n",
    "            self,\n",
    "            request_json: List,\n",
    "    ):\n",
    "        # for gpt-4 / gpt-3.5-turbo, the encoding is \"cl100k_base\"\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        n = 1 # number of completions \n",
    "        completion_tokens = n * 500 # self.max_tokens \n",
    "        # chat completions\n",
    "        num_tokens = 0\n",
    "        try:\n",
    "            for message in request_json:\n",
    "                num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "                for key, value in message.items():\n",
    "                    num_tokens += len(encoding.encode(value))\n",
    "                    if key == \"name\":  # if there's a name, the role is omitted\n",
    "                        num_tokens -= 1  # role is always required and always 1 token\n",
    "            num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "            return num_tokens + completion_tokens\n",
    "        except KeyError:\n",
    "            print(f\"Invalid request JSON: {request_json}\")\n",
    "            return 0\n",
    "        \n",
    "    def handle_last_retry_error(retry_state):\n",
    "        print(f\"All retry attempts failed for: {retry_state.args[0]}\\nReturning None for this request.\")\n",
    "        return None  # Custom behavior after all retries fail\n",
    "\n",
    "    @retry(wait=wait_fixed(2) + wait_random(0, 2),\n",
    "            stop=stop_after_attempt(2),\n",
    "            before_sleep= lambda retry_state: print(\"Retrying...\"),\n",
    "            retry_error_callback=handle_last_retry_error)\n",
    "    def get_response(self, system_user_message: List):\n",
    "        estimated_tokens = self.num_tokens_consumed_from_request(system_user_message)\n",
    "        # Acquire both semaphores to manage requests and tokens\n",
    "        self.request_semaphore.acquire()\n",
    "        try:\n",
    "            self.token_semaphore.acquire(estimated_tokens)  # Acquire tokens\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                            model=self.model_name,\n",
    "                            messages=system_user_message,\n",
    "                            temperature=self.temperature,\n",
    "                            max_tokens=self.max_tokens,\n",
    "                            response_format={\"type\": \"json_object\"},\n",
    "                        )\n",
    "                json_response = response.choices[0].message.content\n",
    "                # comment this line for now to save the results with the new lines for better visualization in the saved dataframe\n",
    "                #json_response = json.loads(json_response)\n",
    "\n",
    "                json.loads(json_response) # Attempt to parse the JSON, will raise an error if not valid JSON\n",
    "                return json_response\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON response for message {system_user_message}\")\n",
    "                raise  # This re-raises the last exception, triggering a retry\n",
    "            except Exception as e:\n",
    "                print(f\"Error while processing: {str(e)}\")\n",
    "                raise\n",
    "            finally:\n",
    "                self.token_semaphore.release(estimated_tokens)  # Release tokens after the request\n",
    "        finally:\n",
    "            self.request_semaphore.release()  # Release request semaphore after handling tokens\n",
    "            time.sleep(60 / self.rate_limit)  # Pause to respect the rate limit to evenly distribute requests over time\n",
    "         \n",
    "    def get_responses_parallel(self, messages_list):\n",
    "        results = []\n",
    "        started = time.time()\n",
    "        with ThreadPoolExecutor(max_workers=self.rate_limit) as executor:\n",
    "            future_to_message = {executor.submit(self.get_response, message): message for message in messages_list}\n",
    "            for future in as_completed(future_to_message):\n",
    "                message = future_to_message[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    # Extract the user's query from the message (response format is a JSON string like '{\"content\": \"4\"}')\n",
    "                    user_query = next((msg['content'] for msg in message if msg['role'] == 'user'), \"Unknown query\")\n",
    "                    results.append({\"input\": user_query, \"content\": result})\n",
    "                except Exception as e:\n",
    "                      results.append({\"input\": \"Error\", \"content\": f\"Error processing message: {str(e)}\"})\n",
    "        print(f\"Total time taken: {time.time() - started} seconds\")\n",
    "        return results \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you stay within the requests and tokens rate limit for the selected model\n",
    "\n",
    "In Our Azure AI settings, gpt-35-turbo has a Request Rate limit (Requests per minute) of 720 and Tokens Rate limit of 120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_turbo_api = APIRequester(model_name = \"gpt-35-turbo\", temperature = 1.0, max_tokens = 20, rate_limit = 100, token_rate_limit = 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 2.0825588703155518 seconds\n"
     ]
    }
   ],
   "source": [
    "results_parallel = gpt35_turbo_api.get_responses_parallel(message_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As u can notice, the order of the responses is not the same as the order of the queries, because the requests are processed in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is 8 + 8?</td>\n",
       "      <td>{\"content\": \"16\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is 3 + 3?</td>\n",
       "      <td>{\"content\": \"6\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is 9 + 9?</td>\n",
       "      <td>{\"content\": \"18\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is 10 + 10?</td>\n",
       "      <td>{\"content\": \"20\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is 7 + 7?</td>\n",
       "      <td>{\"content\": \"14\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is 85 + 85?</td>\n",
       "      <td>{\"content\": \"170\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is 91 + 91?</td>\n",
       "      <td>{\"content\": \"182\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What is 95 + 95?</td>\n",
       "      <td>{\\n  \"content\": \"190\"\\n}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>What is 100 + 100?</td>\n",
       "      <td>{\"content\": \"200\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>What is 80 + 80?</td>\n",
       "      <td>{\"content\": \"160\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input                   content\n",
       "0       What is 8 + 8?         {\"content\": \"16\"}\n",
       "1       What is 3 + 3?          {\"content\": \"6\"}\n",
       "2       What is 9 + 9?         {\"content\": \"18\"}\n",
       "3     What is 10 + 10?         {\"content\": \"20\"}\n",
       "4       What is 7 + 7?         {\"content\": \"14\"}\n",
       "..                 ...                       ...\n",
       "95    What is 85 + 85?        {\"content\": \"170\"}\n",
       "96    What is 91 + 91?        {\"content\": \"182\"}\n",
       "97    What is 95 + 95?  {\\n  \"content\": \"190\"\\n}\n",
       "98  What is 100 + 100?        {\"content\": \"200\"}\n",
       "99    What is 80 + 80?        {\"content\": \"160\"}\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parallel = pd.DataFrame(results_parallel)\n",
    "df_parallel.to_csv(\"results_parallel.csv\", index=False)\n",
    "df_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now a one-per-one request method to compare it with the parallel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIRequesterSimpler:\n",
    "    def __init__(self, model_name, temperature=1.0, max_tokens=15):\n",
    "        self.client = AzureOpenAI(\n",
    "                # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "                api_version=\"2023-07-01-preview\",\n",
    "                # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "               azure_endpoint= \"https://chm-openai.openai.azure.com/\",\n",
    "            )\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def get_response(self, system_user_message: List):\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                                model=self.model_name,\n",
    "                                messages= system_user_message,\n",
    "                                temperature= self.temperature,\n",
    "                                max_tokens=self.max_tokens,\n",
    "                                response_format= { \"type\": \"json_object\" },\n",
    "                            )\n",
    "                    json_response = response.choices[0].message.content\n",
    "                    json.loads(json_response)  # Attempt to parse the JSON, will raise an error if not valid JSON\n",
    "                    return json_response\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Invalid JSON response for message {system_user_message}\\nReturning None for this request.\")\n",
    "                    return '{\"content\": \"None\"}'  # This re-raises the last exception, triggering a retry\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while processing: {str(e)}\")\n",
    "                    raise\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_turbo_api2 = APIRequesterSimpler(model_name = \"gpt-35-turbo\", temperature = 1.0, max_tokens = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 18.58565092086792\n"
     ]
    }
   ],
   "source": [
    "started = time.time()\n",
    "results_2 = []\n",
    "for i in message_sequences:\n",
    "    results_2.append(gpt35_turbo_api2.get_response(i))\n",
    "print(f\"Time taken: {time.time() - started}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can notice, processing these requests one by one took about 18.6 seconds. However, using the parallel processing method, this time was significantly reduced to approximately 2.6 seconds, making it 7 times faster.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debug_coopbot_env_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
